{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Follow the steps to learn how to use the Q Learning algorithm with an E greedy policy approach. </h3>\n",
    "\n",
    "<h4>Part 1: Grid construction and Learning process tunning </h4> Obtain the quality value's table of your grid and verify its coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from e_greedy_q_learning_sensibility import Qlearning, State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.- Define your grid (2D String List) and grid resources dictionary. Ensure it is respected the structure: {string_symbol: [associated_reward, ends_the_episode], ..., additionalInfo}. Where:\n",
    "string_symbol: The string of the symbol to which the information of the value's list is associated.\n",
    "associated: The reward as a float, being positive if suitable or negative if avoidable. \n",
    "ends_the_episode: Boolean type; True if stepping into this symbol (state) ends the episode, False otherwise. Label goal and obstacle symbols for plotting purposes.\n",
    "\n",
    "In the given room example: The fire 'f' is static (potentially avoidable cells). The obstacles are represented as 'o'. Gate, exit or goal of the floor written as 'x'. The empty space is represented as '*'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heigth-> Y = 20      x      width-> X = 20\n",
      "w w w w w w w w w w w w w w w w w w w w\n",
      "w # w w # # # w w # w w w # # # w w w w\n",
      "w # . . # # # . . # w . . # # # . . w w\n",
      "w # . . # # # . . # w . . # # # . . w w\n",
      "w w . . # # # . . w w . . # # # . . w w\n",
      "w w . . # # # . . w w . . # # # . . f d\n",
      "f d . . . . . . . w w . . . . . . . f d\n",
      "f d . . . . . . . w w . . . . . . . w w\n",
      "w w . . . . . . . w w . . . . . . . w w\n",
      "w w . # # # # . . w w . . # # # # . w w\n",
      "w w w # # # # w d d d d w # # # # w w w\n",
      "w w w # # w w w d d d d w w w w w w # w\n",
      "w w . # # . . . . . . . . . . . . . # w\n",
      "w w . # # . . . . . . . . . . . . . w w\n",
      "x d . . . . . . . . . . . . . . . . # w\n",
      "x d . . . . . . . . . . . . . . . . # w\n",
      "w w . . . . . . . . . . . . . . . . # w\n",
      "w w . . . . . . . . . . . . . . . . w w\n",
      "w w # # # # # w w w w w w w # # w # # w\n",
      "w w w w w w w w w w w w w w w w w w w w\n"
     ]
    }
   ],
   "source": [
    "wall_b1 = [(0, 11), (1, 11), (0, 12), (1, 12), (0, 13), (1, 13), (0, 14), (1, 14), (0, 15), (1, 15), (0, 16), (1, 16), (0, 17), (1, 17), (0, 18), (1, 18), (1, 10), (1, 11), (2, 10), (2, 11), (3, 10), (3, 11), (4, 10), (4, 11), (5, 10), (5, 11), (6, 10), (6, 11), (7, 10), (7, 11), (8, 10), (8, 11), (9, 10), (9, 11), (18, 11), (19, 11), (18, 12), (19, 12), (18, 13), (19, 13), (18, 14), (19, 14), (18, 15), (19, 15), (18, 16), (19, 16), (18, 17), (19, 17), (18, 18), (19, 18), (1, 18), (1, 19), (2, 18), (2, 19), (3, 18), (3, 19), (4, 18), (4, 19), (5, 18), (5, 19), (6, 18), (6, 19), (7, 18), (7, 19), (8, 18), (8, 19), (9, 18), (9, 19), (10, 18), (10, 19), (11, 18), (11, 19), (12, 18), (12, 19), (13, 18), (13, 19), (14, 18), (14, 19), (15, 18), (15, 19), (16, 18), (16, 19), (17, 18), (17, 19), (18, 18), (18, 19), (10, 10), (10, 11), (11, 10), (11, 11), (12, 10), (12, 11), (13, 10), (13, 11), (14, 10), (14, 11), (15, 10), (15, 11), (16, 10), (16, 11), (17, 10), (17, 11), (18, 10), (18, 11), (9, 1), (10, 1), (9, 2), (10, 2), (9, 3), (10, 3), (9, 4), (10, 4), (9, 5), (10, 5), (9, 6), (10, 6), (9, 7), (10, 7), (9, 8), (10, 8), (9, 9), (10, 9), (9, 10), (10, 10), (1, 0), (1, 1), (2, 0), (2, 1), (3, 0), (3, 1), (4, 0), (4, 1), (5, 0), (5, 1), (6, 0), (6, 1), (7, 0), (7, 1), (8, 0), (8, 1), (9, 0), (9, 1), (0, 1), (1, 1), (0, 2), (1, 2), (0, 3), (1, 3), (0, 4), (1, 4), (0, 5), (1, 5), (0, 6), (1, 6), (0, 7), (1, 7), (0, 8), (1, 8), (0, 9), (1, 9), (0, 10), (1, 10), (10, 0), (10, 1), (11, 0), (11, 1), (12, 0), (12, 1), (13, 0), (13, 1), (14, 0), (14, 1), (15, 0), (15, 1), (16, 0), (16, 1), (17, 0), (17, 1), (18, 0), (18, 1), (18, 1), (19, 1), (18, 2), (19, 2), (18, 3), (19, 3), (18, 4), (19, 4), (18, 5), (19, 5), (18, 6), (19, 6), (18, 7), (19, 7), (18, 8), (19, 8), (18, 9), (19, 9), (18, 10), (19, 10)]\n",
    "wall_b2 = [(0, 11), (1, 10), (1, 12), (0, 12), (1, 11), (1, 13), (0, 13), (1, 12), (1, 14), (0, 14), (1, 13), (1, 15), (0, 15), (1, 14), (1, 16), (0, 16), (1, 15), (1, 17), (0, 17), (1, 16), (1, 18), (0, 18), (1, 17), (1, 19), (1, 10), (0, 11), (2, 11), (2, 10), (1, 11), (3, 11), (3, 10), (2, 11), (4, 11), (4, 10), (3, 11), (5, 11), (5, 10), (4, 11), (6, 11), (6, 10), (5, 11), (7, 11), (7, 10), (6, 11), (8, 11), (8, 10), (7, 11), (9, 11), (9, 10), (8, 11), (10, 11), (18, 11), (19, 10), (19, 12), (18, 12), (19, 11), (19, 13), (18, 13), (19, 12), (19, 14), (18, 14), (19, 13), (19, 15), (18, 15), (19, 14), (19, 16), (18, 16), (19, 15), (19, 17), (18, 17), (19, 16), (19, 18), (18, 18), (19, 17), (19, 19), (1, 18), (0, 19), (2, 19), (2, 18), (1, 19), (3, 19), (3, 18), (2, 19), (4, 19), (4, 18), (3, 19), (5, 19), (5, 18), (4, 19), (6, 19), (6, 18), (5, 19), (7, 19), (7, 18), (6, 19), (8, 19), (8, 18), (7, 19), (9, 19), (9, 18), (8, 19), (10, 19), (10, 18), (9, 19), (11, 19), (11, 18), (10, 19), (12, 19), (12, 18), (11, 19), (13, 19), (13, 18), (12, 19), (14, 19), (14, 18), (13, 19), (15, 19), (15, 18), (14, 19), (16, 19), (16, 18), (15, 19), (17, 19), (17, 18), (16, 19), (18, 19), (18, 18), (17, 19), (19, 19), (10, 10), (9, 11), (11, 11), (11, 10), (10, 11), (12, 11), (12, 10), (11, 11), (13, 11), (13, 10), (12, 11), (14, 11), (14, 10), (13, 11), (15, 11), (15, 10), (14, 11), (16, 11), (16, 10), (15, 11), (17, 11), (17, 10), (16, 11), (18, 11), (18, 10), (17, 11), (19, 11), (9, 1), (10, 0), (10, 2), (9, 2), (10, 1), (10, 3), (9, 3), (10, 2), (10, 4), (9, 4), (10, 3), (10, 5), (9, 5), (10, 4), (10, 6), (9, 6), (10, 5), (10, 7), (9, 7), (10, 6), (10, 8), (9, 8), (10, 7), (10, 9), (9, 9), (10, 8), (10, 10), (9, 10), (10, 9), (10, 11), (1, 0), (0, 1), (2, 1), (2, 0), (1, 1), (3, 1), (3, 0), (2, 1), (4, 1), (4, 0), (3, 1), (5, 1), (5, 0), (4, 1), (6, 1), (6, 0), (5, 1), (7, 1), (7, 0), (6, 1), (8, 1), (8, 0), (7, 1), (9, 1), (9, 0), (8, 1), (10, 1), (0, 1), (1, 0), (1, 2), (0, 2), (1, 1), (1, 3), (0, 3), (1, 2), (1, 4), (0, 4), (1, 3), (1, 5), (0, 5), (1, 4), (1, 6), (0, 6), (1, 5), (1, 7), (0, 7), (1, 6), (1, 8), (0, 8), (1, 7), (1, 9), (0, 9), (1, 8), (1, 10), (0, 10), (1, 9), (1, 11), (10, 0), (9, 1), (11, 1), (11, 0), (10, 1), (12, 1), (12, 0), (11, 1), (13, 1), (13, 0), (12, 1), (14, 1), (14, 0), (13, 1), (15, 1), (15, 0), (14, 1), (16, 1), (16, 0), (15, 1), (17, 1), (17, 0), (16, 1), (18, 1), (18, 0), (17, 1), (19, 1), (18, 1), (19, 0), (19, 2), (18, 2), (19, 1), (19, 3), (18, 3), (19, 2), (19, 4), (18, 4), (19, 3), (19, 5), (18, 5), (19, 4), (19, 6), (18, 6), (19, 5), (19, 7), (18, 7), (19, 6), (19, 8), (18, 8), (19, 7), (19, 9), (18, 9), (19, 8), (19, 10), (18, 10), (19, 9), (19, 11)]\n",
    "wall_b3 = [(1, 11), (0, 10), (0, 12), (1, 12), (0, 11), (0, 13), (1, 13), (0, 12), (0, 14), (1, 14), (0, 13), (0, 15), (1, 15), (0, 14), (0, 16), (1, 16), (0, 15), (0, 17), (1, 17), (0, 16), (0, 18), (1, 18), (0, 17), (0, 19), (1, 11), (0, 10), (2, 10), (2, 11), (1, 10), (3, 10), (3, 11), (2, 10), (4, 10), (4, 11), (3, 10), (5, 10), (5, 11), (4, 10), (6, 10), (6, 11), (5, 10), (7, 10), (7, 11), (6, 10), (8, 10), (8, 11), (7, 10), (9, 10), (9, 11), (8, 10), (10, 10), (19, 11), (18, 10), (18, 12), (19, 12), (18, 11), (18, 13), (19, 13), (18, 12), (18, 14), (19, 14), (18, 13), (18, 15), (19, 15), (18, 14), (18, 16), (19, 16), (18, 15), (18, 17), (19, 17), (18, 16), (18, 18), (19, 18), (18, 17), (18, 19), (1, 19), (0, 18), (2, 18), (2, 19), (1, 18), (3, 18), (3, 19), (2, 18), (4, 18), (4, 19), (3, 18), (5, 18), (5, 19), (4, 18), (6, 18), (6, 19), (5, 18), (7, 18), (7, 19), (6, 18), (8, 18), (8, 19), (7, 18), (9, 18), (9, 19), (8, 18), (10, 18), (10, 19), (9, 18), (11, 18), (11, 19), (10, 18), (12, 18), (12, 19), (11, 18), (13, 18), (13, 19), (12, 18), (14, 18), (14, 19), (13, 18), (15, 18), (15, 19), (14, 18), (16, 18), (16, 19), (15, 18), (17, 18), (17, 19), (16, 18), (18, 18), (18, 19), (17, 18), (19, 18), (10, 11), (9, 10), (11, 10), (11, 11), (10, 10), (12, 10), (12, 11), (11, 10), (13, 10), (13, 11), (12, 10), (14, 10), (14, 11), (13, 10), (15, 10), (15, 11), (14, 10), (16, 10), (16, 11), (15, 10), (17, 10), (17, 11), (16, 10), (18, 10), (18, 11), (17, 10), (19, 10), (10, 1), (9, 0), (9, 2), (10, 2), (9, 1), (9, 3), (10, 3), (9, 2), (9, 4), (10, 4), (9, 3), (9, 5), (10, 5), (9, 4), (9, 6), (10, 6), (9, 5), (9, 7), (10, 7), (9, 6), (9, 8), (10, 8), (9, 7), (9, 9), (10, 9), (9, 8), (9, 10), (10, 10), (9, 9), (9, 11), (1, 1), (0, 0), (2, 0), (2, 1), (1, 0), (3, 0), (3, 1), (2, 0), (4, 0), (4, 1), (3, 0), (5, 0), (5, 1), (4, 0), (6, 0), (6, 1), (5, 0), (7, 0), (7, 1), (6, 0), (8, 0), (8, 1), (7, 0), (9, 0), (9, 1), (8, 0), (10, 0), (1, 1), (0, 0), (0, 2), (1, 2), (0, 1), (0, 3), (1, 3), (0, 2), (0, 4), (1, 4), (0, 3), (0, 5), (1, 5), (0, 4), (0, 6), (1, 6), (0, 5), (0, 7), (1, 7), (0, 6), (0, 8), (1, 8), (0, 7), (0, 9), (1, 9), (0, 8), (0, 10), (1, 10), (0, 9), (0, 11), (10, 1), (9, 0), (11, 0), (11, 1), (10, 0), (12, 0), (12, 1), (11, 0), (13, 0), (13, 1), (12, 0), (14, 0), (14, 1), (13, 0), (15, 0), (15, 1), (14, 0), (16, 0), (16, 1), (15, 0), (17, 0), (17, 1), (16, 0), (18, 0), (18, 1), (17, 0), (19, 0), (19, 1), (18, 0), (18, 2), (19, 2), (18, 1), (18, 3), (19, 3), (18, 2), (18, 4), (19, 4), (18, 3), (18, 5), (19, 5), (18, 4), (18, 6), (19, 6), (18, 5), (18, 7), (19, 7), (18, 6), (18, 8), (19, 8), (18, 7), (18, 9), (19, 9), (18, 8), (18, 10), (19, 10), (18, 9), (18, 11)]\n",
    "\n",
    "grid_resources = {'.': [-1, False], 'f': [-10, False], 'd': [-1, False], 'w': [-1, False],\n",
    "                  'x': [100, True], 'obstacle':'#', 'goal': 'x', 'b1' : wall_b1, 'b2' : wall_b2, 'b3' :wall_b3}\n",
    "\n",
    "# insert your grid or the needed function to generate it.\n",
    "grid = [['w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w'],\n",
    "        ['w', '#', 'w', 'w', '#', '#', '#', 'w', 'w', '#', 'w', 'w', 'w', '#', '#', '#', 'w', 'w', 'w', 'w'],\n",
    "        ['w', '#', '.', '.', '#', '#', '#', '.', '.', '#', 'w', '.', '.', '#', '#', '#', '.', '.', 'w', 'w'],\n",
    "        ['w', '#', '.', '.', '#', '#', '#', '.', '.', '#', 'w', '.', '.', '#', '#', '#', '.', '.', 'w', 'w'], \n",
    "        ['w', 'w', '.', '.', '#', '#', '#', '.', '.', 'w', 'w', '.', '.', '#', '#', '#', '.', '.', 'w', 'w'], \n",
    "        ['w', 'w', '.', '.', '#', '#', '#', '.', '.', 'w', 'w', '.', '.', '#', '#', '#', '.', '.', 'f', 'd'],\n",
    "        ['f', 'd', '.', '.', '.', '.', '.', '.', '.', 'w', 'w', '.', '.', '.', '.', '.', '.', '.', 'f', 'd'],\n",
    "        ['f', 'd', '.', '.', '.', '.', '.', '.', '.', 'w', 'w', '.', '.', '.', '.', '.', '.', '.', 'w', 'w'], \n",
    "        ['w', 'w', '.', '.', '.', '.', '.', '.', '.', 'w', 'w', '.', '.', '.', '.', '.', '.', '.', 'w', 'w'], \n",
    "        ['w', 'w', '.', '#', '#', '#', '#', '.', '.', 'w', 'w', '.', '.', '#', '#', '#', '#', '.', 'w', 'w'],\n",
    "        ['w', 'w', 'w', '#', '#', '#', '#', 'w', 'd', 'd', 'd', 'd', 'w', '#', '#', '#', '#', 'w', 'w', 'w'],\n",
    "        ['w', 'w', 'w', '#', '#', 'w', 'w', 'w', 'd', 'd', 'd', 'd', 'w', 'w', 'w', 'w', 'w', 'w', '#', 'w'], \n",
    "        ['w', 'w', '.', '#', '#', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '#', 'w'], \n",
    "        ['w', 'w', '.', '#', '#', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'w', 'w'], \n",
    "        ['x', 'd', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '#', 'w'], \n",
    "        ['x', 'd', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '#', 'w'], \n",
    "        ['w', 'w', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '#', 'w'], \n",
    "        ['w', 'w', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'w', 'w'], \n",
    "        ['w', 'w', '#', '#', '#', '#', '#', 'w', 'w', 'w', 'w', 'w', 'w', 'w', '#', '#', 'w', '#', '#', 'w'], \n",
    "        ['w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w', 'w']]\n",
    "\n",
    "# (0,0) in top left corner\n",
    "print(f'heigth-> Y = {len(grid)}      x      width-> X = {len(grid[0])}')\n",
    "for row in grid:\n",
    "    print(' '.join(map(str,row)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Define the initial state from where to start every episode and create the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = State(grid=grid, agent_pos=(2,2))\n",
    "e_greedy_maze = Qlearning(\n",
    "            start_state = start_state,\n",
    "            grid_resources = grid_resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Let's approximate the number of steps per episode maximum value by the largest possible solution to scape from your floor represenation. In my case, ill try with 100 steps. As this value is an estimation, afterwards it will be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episode_steps = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, let's approximate the maximum total number of episodes. I have come to the conclusion in several atempts that this boundary shouldn't surpass the following relationship: number of actions x number of states. Keep in mind that the size of the learning process will always be proportional to the number of different combinations computed from the quantity of possible actions in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_actions = 8 \n",
    "n_episodes = int((n_actions * len(grid) * len(grid[0]))/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the time the learning process lasts markedly varies regarding the hardware it is executed in. Moreover, if you use jupyter notebook or \n",
    "virtual machines, there migth be a slightly software overhead that considerably delays as well the running time. If you want to minimize this period, try running it into your operative system terminal without any intermediary layer.\n",
    "\n",
    "Optionally, you can add more positions in the distributed positions list, in such a way that covers the most general perspectives from where the agent could face\n",
    "the problem. This achieves a more accurate quality value by reinforcing the agent collected experience. In this case, i chose the four corners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0), (1, 2), (0, 1), (2, 1), (2, 2), (0, 0), (2, 0), (0, 2)]\n",
      "[(1, -1), (1, 1), (0, 0), (2, 0), (2, 1), (0, -1), (2, -1), (0, 1)]\n",
      "B3\n",
      "B1\n",
      "B1\n",
      "B1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't go anywhere from cell (1, 0).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-7361b6c54ead>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdistributed_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0me_greedy_maze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent_pos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mlisted_total_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal_reached\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0me_greedy_maze\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgoal_reached\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlisted_total_rewards\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Q-Learning-maze-construction-E-greedy-approach--master 2/e_greedy_q_learning.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, n_episodes, n_episode_steps)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_possible_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                          \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Q-Learning-maze-construction-E-greedy-approach--master 2/e_greedy_q_learning.py\u001b[0m in \u001b[0;36mextract_possible_actions\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Can't go anywhere from cell {x,y}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't go anywhere from cell (1, 0)."
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "i = 1\n",
    "l = dict()\n",
    "\n",
    "distributed_positions = [(1,1), (18,18), (18,1), (1,18)]\n",
    "for x, y in distributed_positions:            \n",
    "    e_greedy_maze.start_state = State(grid=grid, agent_pos=(x,y))\n",
    "    listed_total_rewards, goal_reached  = e_greedy_maze.learn(n_episodes, n_episode_steps)\n",
    "    l[i] = [goal_reached, listed_total_rewards]\n",
    "    i+=1\n",
    "            \n",
    "end = time.time()\n",
    "print(f'#  create_map_qtable > Time to complete:{end - start: .2f}s = {(end - start)/60:.2f} min = {(end - start)/3600:.2f} hours')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following plot, there can be clearly appreciated the velocity with which the learning converges from each of the different starting points. The vertical axis is measured by the total reward at the end of each episode given by a full 'q_table' exploitation policy. Note that as the learning process explores more, the coming cases converge much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.interpolate as interpolate\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "fig.subplots_adjust(top=0.85)\n",
    "fig.tight_layout()\n",
    "n_e = list(range(n_episodes))\n",
    "unicode = '\\u2718'\n",
    "\n",
    "for n,c in l.items():\n",
    "    if c[0]:\n",
    "        unicode = '\\u2713'\n",
    "    spl = interpolate.splrep(n_e, c[1])\n",
    "    x2 = np.linspace(min(n_e), max(n_e), 9)\n",
    "    y2 =  interpolate.splev(x2, spl)\n",
    "    ax.plot(x2, y2, label=f'{n}.- Goal convergence:{unicode}, starting from: {distributed_positions[n-1]}')\n",
    "    ax.legend(loc='best', prop={'size': 20})\n",
    "    plt.title('Consecutive learning processes convergence',  fontsize=30)\n",
    "    plt.xlabel('Nº episodes', fontsize=25)\n",
    "    plt.ylabel('Total reward', fontsize=25)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the 'q_table' has been created by printing a partial sample of it. Remember: q_table = {State (param1, param2, ...) : [q1, q2, q3, q4, q5, q6, q7, q8], ...}. The number of entries in the 'q_table' should be equal to the number of states minus the obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'# number of states - the obstacles = {len(e_greedy_maze.q_table.keys())}. See 3 first samples of those entries:')\n",
    "dict(list(e_greedy_maze.q_table.items())[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Store the 'q-table' and its respective grid in the 'qlearning_files' folder, to enable the inference in further situations, without needing to loose time learning again. Verify that the folders are already created. Otherwise, create them manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_id = 'id1'\n",
    "qtable_file = './qlearning_files/pickles/'+str_id+'_qtable'\n",
    "grid_file = './qlearning_files/grids/'+str_id+'_grid'\n",
    "e_greedy_maze.convert_to_pickle(e_greedy_maze.q_table, qtable_file)\n",
    "e_greedy_maze.convert_to_pickle(grid, grid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.- Now, those files should appear in the mentioned folders. Let's try a simulacrum of recovering the 'q-table'-grid pair from the pickle stored files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_id = 'id1'\n",
    "qtable_file = './qlearning_files/pickles/'+str_id+'_qtable'\n",
    "q_table = e_greedy_maze.extract_from_pickle(qtable_file)\n",
    "grid_file = './qlearning_files/grids/'+str_id+'_grid'\n",
    "grid = e_greedy_maze.extract_from_pickle(grid_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify its integrity by comparing it with the dictionary's head computed before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(list(e_greedy_maze.q_table.items())[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify there are no unexplored states. That is, the following heating plot must not show any black cell, except the ones\n",
    "containing an obstacle. Additionally, note that the clearer the colour is, the safest that zone is, and the darker the colour is, the more dangerous that zone is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy_maze.visualize_max_quality_action (q_table, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look to the coherence of the resulted state quality values, according to their directions. Wherever you start, the arrows flow should guide you into the black circle (defined as goal in grid resources parameter) as if it were a draining gravity force. Squares are obstacles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_greedy_maze.q_value_ascii_action (q_table, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Part 2: Optimal solution inferring: </h4> Infer the best route and verify its suitability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different start states from where to infer the optimal route. Logically, the correct path should reach the goal cell avoiding the dangers through the shortest number of states. Note that we first have to take back certain information to give to the QLearning class. The large info: grid and the 'q_table' are already recovered from the pickle file. The short info: grid resources can be easily remembered. Path symbol: ☸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_resources = {'*': [-1, False], 'f': [-10, False],\n",
    "                  'x': [100, True], 'obstacle':'o', 'goal': 'x'}\n",
    "str_id = 'id1'\n",
    "inference_state = State(grid=grid, agent_pos=(18,1))\n",
    "e_greedy_maze = Qlearning(\n",
    "    inference_state = inference_state,\n",
    "    grid_resources = grid_resources )\n",
    "e_greedy_maze.q_table = q_table\n",
    "            \n",
    "path, total_reward = e_greedy_maze.infer_path(n_episode_steps, inference_state)\n",
    "\n",
    "e_greedy_maze.visualize_inferenced_path(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_// That's it. If you found this useful, remember that you can 'like' this project by giving me a star on its Github repo._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
